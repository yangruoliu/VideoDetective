# ============================================================
# VideoDetective Configuration Example
# Copy this file to .env and fill in your settings
# ============================================================

# ===== Required: VLM API Settings =====
# Your API key for the Vision-Language Model service
# Supports OpenAI-compatible APIs (e.g., DashScope, OpenRouter, etc.)
VIDEODETECTIVE_API_KEY=your_api_key_here

# API base URL (default: DashScope for Qwen models)
# Examples:
#   - DashScope: https://dashscope.aliyuncs.com/compatible-mode/v1
#   - OpenAI: https://api.openai.com/v1
#   - OpenRouter: https://openrouter.ai/api/v1
VIDEODETECTIVE_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# VLM model name for multimodal understanding
# Examples:
#   - qwen3-vl-8b-instruct (recommended)
#   - qwen-vl-max
#   - gpt-4o
VIDEODETECTIVE_VLM_MODEL=qwen3-vl-8b-instruct

# ===== Optional: Text LLM Settings =====
# If not set, falls back to VLM settings
# VIDEODETECTIVE_LLM_MODEL=qwen3-8b
# VIDEODETECTIVE_LLM_API_KEY=your_text_llm_api_key_here
# VIDEODETECTIVE_LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# ===== Optional: Encoder Settings =====
# NOTE: SigLIP and CLIP models are pre-included in the models/ directory.
# You don't need to download them separately.

# SigLIP model ID (default uses the model in models/ directory)
# SIGLIP_MODEL_ID=google/siglip-so400m-patch14-384

# HuggingFace cache directory (default: ./models)
# HF_CACHE_DIR=/path/to/your/cache

# CLIP fallback path (default: ./models/models--openai--clip-vit-base-patch32)
# CLIP_LOCAL_PATH=/path/to/clip/model

# HuggingFace mirror (for users in China, only needed if downloading new models)
# HF_ENDPOINT=https://hf-mirror.com

# ===== Optional: Pipeline Settings =====
# Maximum frames per VLM call (default: 64)
# VIDEODETECTIVE_MAX_FRAMES_PER_CALL=64

# Enable multi-route recall for better semantic understanding (default: true)
# ENABLE_MULTI_ROUTE_RECALL=true

# Use VLM-based relevance scoring (default: false, uses text similarity instead)
# USE_VLM_RELEVANCE=false

# Include evidence text in final answer generation (default: true)
# INCLUDE_ANSWER_EVIDENCE=true

# ===== Optional: ASR (Speech Recognition) Settings =====
# Enable/disable ASR extraction (default: true)
# VIDEODETECTIVE_ENABLE_ASR=true

# Whisper model to use: tiny, base, small, medium, large (default: base)
# Larger models are more accurate but slower and use more memory
# - tiny: ~39M params, fastest, least accurate
# - base: ~74M params, good balance (recommended)
# - small: ~244M params, better accuracy
# - medium: ~769M params, high accuracy
# - large: ~1550M params, best accuracy
# VIDEODETECTIVE_WHISPER_MODEL=base

# Device for ASR processing (default: auto-detect)
# VIDEODETECTIVE_ASR_DEVICE=cuda
